
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Object-Centric Instruction Augmentation for Robotic Manipulation</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">


    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <font size="+4"><b>OCI-Robotics</b>:</font> </br> Object-Centric Instruction Augmentation for Robotic Manipulation</br> 
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
               <ul class="list-inline">
                <br>

<li>Junjie Wen<sup>1,*</sup></li> <li>Yichen Zhu</sup>2,*<sup></li> <li>Minjie Zhu<sup>1</sup></li> <li>Jinming Li<sup>3</sup></li> <li>Zhiyuan Xu<sup>2</sup></li> 
<li>Zhengping Che<sup>2</sup></li> <br><br><li>Chaomin Shen<sup>1,&dagger;</sup></li> <li>Yaxin Peng<sup>3</sup></li><li>Dong Liu<sup>2</sup></li> <li>Feifei Feng<sup>2</sup></li> 
<li>and Jian Tang<sup>2,&dagger;</sup></li> 
              
                <br><br>
                    <!-- <a href="http://g.co/robotics">
                    <img src="img/robotics-at-google.png" height="40px"> Robotics at Google</a> <br> -->
                    <h5><sup>1</sup> School of Computer Science, East China Normal University, China</h5>
                    <h5><sup>2</sup> Midea Group, China</h5>
                    <h5><sup>3</sup> Department of Mathematics, School of Science, Shanghai University, China</h5>
                    <h5> * Equal contribution.This work was done during Junjie Wen's, Minjie Zhu's, and Jinming Li's internship at Midea Group. </h5>
                    <h5>Accepted by 2024 IEEE International Conference on Robotics and Automation (ICRA)</h5>
                </ul>
            </div>
        </div>

        
        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2401.02814">
                            <image src="img/paper_icon.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/vjh1PAn13sA">
                                <image src="img/youtube_icon.png" height="60px">
                                    <h4><strong>Video</strong></h4>
                                </a>
                            </li>
                                </ul>
                                
                            </div>
                        </div>
                        
                        
                        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <video id="v0" width="100%" playsinline loop controls muted autoplay>
                        <source src="img/icra2024_oci.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                                
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Humans interpret scenes by recognizing both the identities and positions of objects in their observations. For a robot to perform tasks such as “pick and place”, understanding both what the objects are and where they are located is crucial. While the former has been extensively discussed in the literature that uses the large language model to enrich the text descriptions, the latter remains underexplored. In this work, we introduce the Object-Centric Instruction Augmentation (OCI) framework. We utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of object locations into natural language instruction, thus aiding the policy network in mastering actions for versatile manipulation. Additionally, we present a feature reuse mechanism to integrate the vision- language features from off-the-shelf pre-trained MLLM into policy networks. Through a series of simulated and real-world robotic tasks, we demonstrate that robotic manipulator imita- tion policies trained with our enhanced instructions outperform those relying solely on traditional language instructions.
                </p>

                <!-- <div class="text-center">
                    <video id="v0" width="100%" playsinline loop controls muted autoplay>
                        <source src="img/im_teaser_compressed.mp4" type="video/mp4">
                   </video>
                </div> -->
            
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Method
                </h3>
                <p class="text-justify">
                    Prior works have studied the use of pre-trained Vision-Language Models (VLM) for robotics. Some works use pre-trained VLM as instruction 
                    encoder or visual encoder. These prioapproaches use VLMs for visual state representations, for object navigation, for high-level planning 
                    or for providing supervision or success detection . For instance, SuccessVQA uses VLM to detect whether the behavior is successful. 
                    LMNAV does long-horizon navigation through out- door scenes from natural language instruction. Voltron presents a language-conditioned 
                    visual representation from human videos. CLIPort and MOO, which integrate pre-trained VLMs into end-to-end visuomotor manipulation policies. 
                     RT-2 develop an end-to-end framework that outputs actions with images and instructions directly.<br>
                    Unlike prior applications of foundational models to down- stream tasks like planning and navigation, our goal is to enhance language 
                    instructions to boost the generalizability of robotic manipulation, emphasizing positional cues for objects in text structure.There are two key 
                    components to our OCI framework: 1) a fine-tuned MLLM that is adept at comprehending language and the environment, with the ability to 
                    correlate an object’s location to its identity, and 2) a feature reuse mechanism that leverages the features embedding from MLLM to 
                    improve policy learning. <br>

              
                <image src="img/oci_framework.png" width="100%" height="auto">
                <p class="text-justify">
                    Two examples of object-centric instruction augmentation for simulation and real robots, respectively. Given an initial instruction from the left figure, we augment them by providing the object’s absolute position and relative position to the robotics and obtain the action eventually.
                </p>
                <image src="img/example_method.png" width="100%" height="auto">
            

            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experiments
                </h3>
		<p class="text-justify">
            Our experiments aim to answer the following questions: 1) Does our method enable better policy learning than using naive language instruction? 2) Is our method effective with real-world environments? We initiate our discussion by outlining the simulation environments tailored to address these queries. Subsequently, we present in-depth experimen- tal results that positively affirm answers to both questions.
		</p>

        <h4>
            Simulated Franka Kitchen Environment
        </h4>
        <p class="text-justify">
            The example of Franka Kitchen for five tasks on two camera views.
        </p>
        <image src="img/example_franka.png" width="100%" height="auto">

        <p class="text-justify">
            We compare our model with R3M, which is the state-of-the-art method and widely applicable method in Franka Kitchen. We also compare with BLIP-2, a SOTA vision-language model. We replace our MLLM with BLIP-2 in our method and retain the FRM(Feature Reuse Module).And results are shown in the following figure.
        </p>
        <image src="img/result_franka.png" width="100%" height="auto">


        <h4>
            Real-World Environment
        </h4>
         <p class="text-justify">
            We use the Franka robot with a 7-degree of freedom arm, which is equipped with a parallel jaw gripper (see following figure, left). Our workspace boasts two high-quality D435i RealSense RGBD cameras. We only use the RGB information in our experiments.
            The real robot setup is on the left and examples of tasks are on the right.
        </p>
        <image src="img/example_real.png" width="100%" height="auto">
        <p class="text-justify">
            Results from real-world experiments.In this study, we progressively removed the relative position (denoted as “w/o rel-pos”) and the absolute position (denoted as “w/o abs-pos”).
        </p>
        <img src="img/result_real.png" style="width: 60%; height: auto; display: block; margin: auto;">
    </div>
    <div class="row">
        <div class="col-md-8 col-md-offset-2">
                    <h3>
                        Citation
                    </h3>
                    <div class="form-group col-md-10 col-md-offset-1">
                        <textarea id="bibtex" class="form-control" readonly="" style="display: none;">@inproceedings{wen2023oci,
        title={Object-Centric Instruction Augmentation for Robotic Manipulation},
        author={Junjie Wen and Yichen Zhu and Minjie Zhu and Jinming Li and Zhiyuan Xu and Zhengping Che and Chaomin Shen and Yaxin Peng and Dong Liu and Feifei Feng and Jian Tang},
        booktitle={International Conference on Robotics and Automation (ICRA)},
        year={2024}
    }</textarea>
            </div>
    </div>
    </div>
</body>
</html>
