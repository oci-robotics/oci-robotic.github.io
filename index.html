
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Object-Centric Instruction Augmentation for Robotic Manipulation</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta name="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image" content="https://innermonologue.github.io/img/teaser.png" />
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="2000">
    <meta property="og:image:height" content="900">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://inner-monologue.github.io/"/>
    <meta property="og:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta property="og:description" content="Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Inner Monologue: Embodied Reasoning through Planning with Language Models." />
    <meta name="twitter:description" content="Project page for Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />
    <meta name="twitter:image" content="https://innermonologue.github.io/img/teaser.png" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <font size="+4"><b>OCI-Robotics</b>:</font> </br> Object-Centric Instruction Augmentation for Robotic Manipulation</br> 
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
               <ul class="list-inline">
                <br>

<li>Junjie Wen<sup>1,*</sup></li> <li>Yichen Zhu</sup>2,*<sup></li> <li>Minjie Zhu<sup>1</sup></li> <li>Jinming Li<sup>3</sup></li> <li>Zhiyuan Xu<sup>2</sup></li> 
<li>Zhengping Che<sup>2</sup></li> <br><br><li>Chaomin Shen<sup>1,&commat</sup></li> <li>Yaxin Peng<sup>3</sup></li><li>Dong Liu<sup>2</sup></li> <li>Feifei Feng<sup>2</sup></li> 
<li>and Jian Tang<sup>2,&commat</sup></li> 
              
                <br><br>
                    <!-- <a href="http://g.co/robotics">
                    <img src="img/robotics-at-google.png" height="40px"> Robotics at Google</a> <br> -->
                    <h5><sup>1</sup> School of Computer Science, East China Normal University, China</h5>
                    <h5><sup>2</sup> Midea Group, China</h5>
                    <h5><sup>3</sup> Department of Mathematics, School of Science, Shanghai University, China</h5>
                    <h5> * Equal contribution.This work was done during Junjie Wen's, Minjie Zhu's, and Jinming Li's internship at Midea Group. </h5>
                </ul>
            </div>
        </div>

        
        <div class="row">
            <div class="col-md-4 col-md-offset-4 text-center">
                <ul class="nav nav-pills nav-justified">
                    <li>
                        <a href="https://arxiv.org/abs/2207.05608">
                            <image src="img/paper_icon.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://youtu.be/0sJjdxn5kcI">
                                <image src="img/youtube_icon.png" height="60px">
                                    <h4><strong>Video</strong></h4>
                                </a>
                            </li>
                            <!-- li>
                                <a href="https://ai.googleblog.com/2021/04/multi-task-robotic-reinforcement.html">
                                    <image src="img/google-ai-blog-small.png" height="60px">
                                        <h4><strong>Blogpost</strong></h4>
                                    </a>
                                </li -->
                                <!-- <li>
                                    <a href="https://github.com/">
                                        <image src="img/github.png" height="60px">
                                            <h4><strong>Code</strong></h4>
                                        </a>
                                    </li> -->
                                </ul>
                                
                            </div>
                        </div>
                        
                        
                        
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <video id="v0" width="100%" playsinline loop controls muted autoplay>
                        <source src="img/icra2024_oci.mp4" type="video/mp4">
                    </video>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                                
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
                    Humans interpret scenes by recognizing both the identities and positions of objects in their observations. For a robot to perform tasks such as “pick and place”, understanding both what the objects are and where they are located is crucial. While the former has been extensively discussed in the literature that uses the large language model to enrich the text descriptions, the latter remains underexplored. In this work, we introduce the Object-Centric Instruction Augmentation (OCI) framework. We utilize a Multi-modal Large Language Model (MLLM) to weave knowledge of object locations into natural language instruction, thus aiding the policy network in mastering actions for versatile manipulation. Additionally, we present a feature reuse mechanism to integrate the vision- language features from off-the-shelf pre-trained MLLM into policy networks. Through a series of simulated and real-world robotic tasks, we demonstrate that robotic manipulator imita- tion policies trained with our enhanced instructions outperform those relying solely on traditional language instructions.
                </p>

                <!-- <div class="text-center">
                    <video id="v0" width="100%" playsinline loop controls muted autoplay>
                        <source src="img/im_teaser_compressed.mp4" type="video/mp4">
                   </video>
                </div> -->
            
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Method
                </h3>
                <p class="text-justify">
                    Prior works have studied the use of pre-trained Vision-Language Models (VLM) for robotics. Some works use pre-trained VLM as instruction 
                    encoder or visual encoder. These prioapproaches use VLMs for visual state representations, for object navigation, for high-level planning 
                    or for providing supervision or success detection . For instance, SuccessVQA uses VLM to detect whether the behavior is successful. 
                    LMNAV does long-horizon navigation through out- door scenes from natural language instruction. Voltron presents a language-conditioned 
                    visual representation from human videos. CLIPort and MOO, which integrate pre-trained VLMs into end-to-end visuomotor manipulation policies. 
                     RT-2 develop an end-to-end framework that outputs actions with images and instructions directly.<br>
                    Unlike prior applications of foundational models to down- stream tasks like planning and navigation, our goal is to enhance language 
                    instructions to boost the generalizability of robotic manipulation, emphasizing positional cues for objects in text structure.There are two key 
                    components to our OCI framework: 1) a fine-tuned MLLM that is adept at comprehending language and the environment, with the ability to 
                    correlate an object’s location to its identity, and 2) a feature reuse mechanism that leverages the features embedding from MLLM to 
                    improve policy learning. The following section will address these two challenges in detail.

              
                <image src="img/oci_framework.png" width="100%" height="auto">
            
            

            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Experiments
                </h3>
		<p class="text-justify">
            Our experiments aim to answer the following questions: 1) Does our method enable better policy learning than using naive language instruction? 2) Is our method effective with real-world environments? We initiate our discussion by outlining the simulation environments tailored to address these queries. Subsequently, we present in-depth experimen- tal results that positively affirm answers to both questions.
		</p>

        <h4>
            Simulated Franka Kitchen Environment
        </h4>
        <p class="text-justify">
            The example of Franka Kitchen for five tasks on two camera views.
        </p>
        <image src="img/example_franka.png" width="100%" height="auto">

        <p class="text-justify">
            We compare our model with R3M, which is the state-of-the-art method and widely applicable method in Franka Kitchen. We also compare with BLIP-2, a SOTA vision-language model. We replace our MLLM with BLIP-2 in our method and retain the FRM(Feature Reuse Module).And results are shown in the following figure.
        </p>
        <image src="img/result_franka.png" width="100%" height="auto">


        <h4>
            Real-World Environment
        </h4>
         <p class="text-justify">
            We use the Franka robot with a 7-degree of freedom arm, which is equipped with a parallel jaw gripper (see following figure, left). Our workspace boasts two high-quality D435i RealSense RGBD cameras. We only use the RGB information in our experiments.
            The real robot setup is on the left and examples of tasks are on the right.
        </p>
        <image src="img/example_real.png" width="100%" height="auto">
        <p class="text-justify">
            Results from real-world experiments.In this study, we progressively removed the relative position (denoted as “w/o rel-pos”) and the absolute position (denoted as “w/o abs-pos”).
        </p>
        <image src="img/result_real.png" width="100%" height="auto">
            
     <div class="col-md-8 col-md-offset-2">
                <h3>
                    Citation
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly="" style="display: none;">@inproceedings{huang2022inner,
    title={Inner Monologue: Embodied Reasoning through Planning with Language Models},
    author={Wenlong Huang and Fei Xia and Ted Xiao and Harris Chan and Jacky Liang and Pete Florence and Andy Zeng and Jonathan Tompson and Igor Mordatch and Yevgen Chebotar and Pierre Sermanet and Noah Brown and Tomas Jackson and Linda Luu and Sergey Levine and Karol Hausman and Brian Ichter},
    booktitle={arXiv preprint arXiv:2207.05608},
    year={2022}
}</textarea>
            </div>
     </div>

     <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
The authors would like to thank Kanishka Rao and Vincent Vanhoucke for valuable feedback and discussions. In addition, the authors would like to acknowledge the large team who built SayCan, upon which we construct our Kitchen Mobile Manipulation experiments.
                    <br><br>
                The website template was borrowed from <a href="http://jonbarron.info/">Jon Barron</a>.
                </p>
            </div>

    </div>
</body>
</html>
